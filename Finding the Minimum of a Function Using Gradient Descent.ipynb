{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/6JR43Z6iRI+vOYDhYy6f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdNnxYreJcoJ"
      },
      "outputs": [],
      "source": [
        "def grad_descent_v1(f, deriv, x0=None, lr=0.1, iters=1000, callback=None):\n",
        "    \"\"\"\n",
        "    Implementation of gradient descent for functions with a single local minimum\n",
        "    that coincides with the global minimum. All tests will have this nature.\n",
        "    :param func: float -> float — the function\n",
        "    :param deriv: float -> float — its derivative\n",
        "    :param x0: float — the initial point\n",
        "    :param lr: float — learning rate\n",
        "    :param iters: int — number of iterations\n",
        "    :param callback: callable — logging function\n",
        "    \"\"\"\n",
        "    if x0 is None:\n",
        "        x0 = np.random.uniform()  # If no initial point is provided, select a random point.\n",
        "\n",
        "    x = x0\n",
        "    callback(x, f(x))  # Call the callback function with the initial point and its function value.\n",
        "\n",
        "    for i in range(iters):\n",
        "        x = x - lr * deriv(x)  # Update the current point by moving against the gradient.\n",
        "        callback(x, f(x))  # Call the callback function with the updated point and its function value.\n",
        "\n",
        "    return x  # Return the point found as the minimum."
      ]
    }
  ]
}